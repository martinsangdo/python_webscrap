{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pymongo\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from math import ceil\n",
    "import csv\n",
    "import importlib\n",
    "# Get the current working directory of the notebook\n",
    "notebook_dir = os.getcwd()\n",
    "# Get the path to the parent directory\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "\n",
    "# Add the parent directory to sys.path if it's not already there\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "import const\n",
    "# importlib.reload(const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True) \n",
    "GENERATIVE_URI = os.environ['GENERATIVE_URI']\n",
    "db_client = pymongo.MongoClient(os.environ['DB_URI'])\n",
    "db = db_client['db_certificates']   \n",
    "metadata_collection = db['tb_cert_metadata']    #meta data of certificates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(GENERATIVE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLE_PROMPT = os.environ['ROLE_PROMPT']\n",
    "COMMON_QUESTION_PROMPT = os.environ['COMMON_QUESTION_PROMPT']\n",
    "MULTI_CHOICE_PROMPT = COMMON_QUESTION_PROMPT + os.environ['MULTI_CHOICE_PROMPT']\n",
    "MULTI_SELECTION_PROMPT = COMMON_QUESTION_PROMPT + os.environ['MULTI_SELECTION_PROMPT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPEN_ROUTER_AI_KEY=os.environ['OPENROUTER_KEY']\n",
    "#platform = 'OPENROUTER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_questions_2_db(platform, collection, raw_questions, question_type):\n",
    "    questions = const.extract_questions_from_candidates(platform, raw_questions)\n",
    "    if questions:\n",
    "        #parse questions and answers\n",
    "        question_num = 0\n",
    "        for q in questions:\n",
    "            if question_type == 'multiple-choice' or (len(q['answer']) > 1):\n",
    "                q['exported'] = 0\n",
    "                q['uuid'] = const.generate_random_uuid()\n",
    "                #print(q)\n",
    "                if 'explanation' in q and 'answer' in q and 'question' in q and 'options' in q:\n",
    "                    if 'A' in q['explanation'] and 'A' in q['options']:\n",
    "                        const.insert_questions(collection, q)\n",
    "                        question_num += 1\n",
    "        print('Stored ' + str(question_num) + ' questions to db successfully')\n",
    "    else:\n",
    "        print(\"Error: No questions found in the parsed content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(cert_metadata, platform):\n",
    "    if 'prompt_context' not in cert_metadata:\n",
    "        print('Missing prompt_context')\n",
    "        return\n",
    "    context = cert_metadata['prompt_context']\n",
    "    exam_name = cert_metadata['name']\n",
    "\n",
    "    question_collection = db[cert_metadata['collection_name']]\n",
    "    exceeded_quota = False\n",
    "    #multiple choice\n",
    "    if 'multi_choice_prompt_prefix' in cert_metadata:\n",
    "        text_prompt = cert_metadata['multi_choice_prompt_prefix'].replace('{exam_name}', exam_name) + MULTI_CHOICE_PROMPT\n",
    "        final_prompt = ROLE_PROMPT + context + text_prompt\n",
    "        ################\n",
    "        print(final_prompt)\n",
    "        return\n",
    "        ################\n",
    "        no_of_loop = ceil(cert_metadata['multi_choice_questions'] / 10)\n",
    "        for i in range(no_of_loop):\n",
    "            if platform is None or platform == '':\n",
    "                #default is Gemini\n",
    "                raw_generated_text = const.post_request_generative_ai(GENERATIVE_URI, final_prompt)\n",
    "                if 'error' in raw_generated_text and 'message' in raw_generated_text['error']:\n",
    "                    if raw_generated_text['error']['message'].find('You exceeded your current quota') >= 0:\n",
    "                        print('You exceeded your current quota, pls try other key or wait until next day')\n",
    "                        exceeded_quota = True\n",
    "                        break\n",
    "            elif platform == 'OPENROUTER':\n",
    "                raw_generated_text = const.send_raw_request_2_openrouter(final_prompt, OPEN_ROUTER_AI_KEY)\n",
    "            \n",
    "            store_questions_2_db(platform, question_collection, raw_generated_text, 'multiple-choice')\n",
    "            time.sleep(5)   #delay 5 seconds\n",
    "    #multi selection, if any\n",
    "    if exceeded_quota == False and 'multi_selection_prompt_prefix' in cert_metadata:\n",
    "        text_prompt = cert_metadata['multi_selection_prompt_prefix'].replace('{exam_name}', exam_name) + MULTI_SELECTION_PROMPT\n",
    "        final_prompt = ROLE_PROMPT + context + text_prompt\n",
    "        no_of_loop = ceil(cert_metadata['multi_selection_questions'] / 10)\n",
    "        for i in range(no_of_loop):\n",
    "            raw_generated_text = const.post_request_generative_ai(GENERATIVE_URI, final_prompt)\n",
    "            if 'error' in raw_generated_text and 'message' in raw_generated_text['error']:\n",
    "                if raw_generated_text['error']['message'].find('You exceeded your current quota') >= 0:\n",
    "                    print('You exceeded your current quota, pls try other key or wait until next day')\n",
    "                    break\n",
    "            store_questions_2_db(question_collection, raw_generated_text, 'multiple-selection')\n",
    "            time.sleep(5)   #delay 5 seconds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def begin_generate_questions(cert_symbol, platform, no_of_tests):\n",
    "    if cert_symbol is None or cert_symbol == '':\n",
    "        return\n",
    "    #query metadata of this symbol\n",
    "    cert_metadata = metadata_collection.find_one({'symbol': cert_symbol})\n",
    "    if cert_metadata is None:\n",
    "        print('Certificate not found')\n",
    "        return\n",
    "    print('Begin generating questions for: ' + cert_metadata['name'])\n",
    "    #\n",
    "    for i in range(no_of_tests):\n",
    "        generate_questions(cert_metadata, platform)\n",
    "        print(cert_symbol + ' ========== Finish generating set: ' + str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_csv(cert_metadata, test_set_number):\n",
    "    question_collection = db[cert_metadata['collection_name']]\n",
    "    file_path = './'+cert_metadata['collection_name']+'/'\n",
    "    #get questions that not exported yet. Note that: each part must follow by domain percents\n",
    "    file_data = []\n",
    "    #append header line (both multi-choice and multi-selection)\n",
    "    file_data.append(['Question','Question Type','Answer Option 1','Explanation 1','Answer Option 2','Explanation 2','Answer Option 3','Explanation 3','Answer Option 4','Explanation 4','Answer Option 5','Explanation 5','Answer Option 6','Explanation 6','Correct Answers','Overall Explanation','Domain'])\n",
    "    exported_uuid = []\n",
    "    manual_uuid = []\n",
    "    #1. export multiple-choice first\n",
    "    pipeline = [\n",
    "                {\"$match\": {'exported': 0, 'type': 'multiple-choice'}},\n",
    "                {\"$sample\": {\"size\": cert_metadata['multi_choice_questions']}}\n",
    "            ]\n",
    "    random_documents = list(question_collection.aggregate(pipeline))\n",
    "    for doc in random_documents:\n",
    "        # print(doc)\n",
    "        if 'D' not in doc['options']:\n",
    "            print(doc['options'])\n",
    "        file_data.append([doc['question'].replace('  ', ' ').replace('\\n', ''), 'multiple-choice', \n",
    "                                  doc['options']['A'], doc['explanation']['A'].replace('  ', ' ').replace('\\n', ''),     #A\n",
    "                                  doc['options']['B'], doc['explanation']['B'].replace('  ', ' ').replace('\\n', ''),     #B\n",
    "                                  doc['options']['C'], doc['explanation']['C'].replace('  ', ' ').replace('\\n', ''),     #C\n",
    "                                  doc['options']['D'], doc['explanation']['D'].replace('  ', ' ').replace('\\n', ''),     #D\n",
    "                                  '', '',   #E\n",
    "                                  '', '',   #6\n",
    "                                  const.map_index(doc['answer']), #correct answer\n",
    "                                  '', #overall\n",
    "                                  '' #domain\n",
    "                                  ])\n",
    "        exported_uuid.append(doc['uuid'])\n",
    "    #2. multi selection\n",
    "    if 'multi_selection_questions' in cert_metadata and cert_metadata['multi_selection_questions'] > 0:\n",
    "        pipeline = [\n",
    "                    {\"$match\": {'exported': 0, 'type': 'multiple-selection'}},\n",
    "                    {\"$sample\": {\"size\": cert_metadata['multi_selection_questions']}}\n",
    "                ]\n",
    "        random_documents = list(question_collection.aggregate(pipeline))\n",
    "        for doc in random_documents:\n",
    "            exported_uuid.append(doc['uuid'])\n",
    "            manual_uuid.append(doc['uuid']) #they do not suppor bulk upload this type of question, we need to manually add them\n",
    "    #save all questions to csv\n",
    "    filename = cert_metadata['csv_filename_prefix']+test_set_number+'.csv'\n",
    "    try:\n",
    "        with open(file_path + filename, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerows(file_data)\n",
    "            print(f\"Data successfully saved to '{file_path}{filename}'\")\n",
    "            for _id in exported_uuid:\n",
    "                question_collection.update_one({'uuid': _id}, {'$set': {'exported': 1, 'filename': filename}})\n",
    "            print('\",\"'.join(manual_uuid))\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the array: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export 1 test at once\n",
    "def begin_export_csv(cert_symbol, test_set_number):\n",
    "    if cert_symbol is None or cert_symbol == '':\n",
    "        return\n",
    "    cert_metadata = metadata_collection.find_one({'symbol': cert_symbol})\n",
    "    if cert_metadata is None:\n",
    "        print('Certificate not found')\n",
    "        return\n",
    "    #\n",
    "    export_csv(cert_metadata, test_set_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TERRA_A_004\n",
      "tb_terra_a_004\n",
      "========\n",
      "AZ_DP_300\n",
      "tb_az_dp_300\n",
      "========\n",
      "AZ_AZ_900\n",
      "tb_az_az_900\n",
      "========\n",
      "AZ_AZ_700\n",
      "tb_az_az_700\n",
      "========\n",
      "AZ_AZ_500\n",
      "tb_az_az_500\n",
      "========\n"
     ]
    }
   ],
   "source": [
    "#set flag to exams that have incorrect exported test\n",
    "def find_wrong_exams():\n",
    "    certs = metadata_collection.find({'udemy_link': {'$ne':None}})\n",
    "    for cert in certs:\n",
    "        question_collection = db[cert['collection_name']]\n",
    "        exported_questions = question_collection.find({'exported': 1})\n",
    "        if len(list(exported_questions)) < 240:\n",
    "            print(cert['symbol'])\n",
    "            print(cert['collection_name'])\n",
    "            print('========')\n",
    "#test\n",
    "find_wrong_exams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicated questions by UUID\n",
    "def remove_duplicated_questions_by_uuid(collection_name):\n",
    "    question_collection = db[collection_name]\n",
    "    questions = question_collection.find({})    #find all\n",
    "    uuid_map = {}   #duplicated uuids, key: uuid, value: 0/1 \n",
    "    for question in questions:\n",
    "        if question['uuid'] not in uuid_map:\n",
    "            #first appear\n",
    "            uuid_map[question['uuid']] = 1\n",
    "        else:\n",
    "            #this UUID is duplicated\n",
    "            uuid_map[question['uuid']] = uuid_map[question['uuid']] + 1\n",
    "    #\n",
    "    for uuid in uuid_map:\n",
    "        if uuid_map[uuid] > 1:\n",
    "            #remove duplicated question\n",
    "            question_collection.delete_many({'uuid':uuid})\n",
    "\n",
    "#test\n",
    "#remove_duplicated_questions_by_uuid('tb_az_az_500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicated questions by question\n",
    "def remove_duplicated_questions_by_question(collection_name):\n",
    "    question_collection = db[collection_name]\n",
    "    questions = question_collection.find({})    #find all\n",
    "    question_map = {}   #duplicated uuids, key: question, value: 0/1 \n",
    "    for question in questions:\n",
    "        if question['question'] not in question_map:\n",
    "            #first appear\n",
    "            question_map[question['question']] = 1\n",
    "        else:\n",
    "            #this UUID is duplicated\n",
    "            question_map[question['question']] = question_map[question['question']] + 1\n",
    "    #\n",
    "    for question in question_map:\n",
    "        if question_map[question] > 1:\n",
    "            print(question)\n",
    "            #remove duplicated question\n",
    "            question_collection.delete_many({'question':question})\n",
    "\n",
    "#test\n",
    "remove_duplicated_questions_by_question('tb_terra_a_004')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin generating questions for: HashiCorp Certified: Terraform Associate 004\n",
      "You are an expert in IT fields who can design examination questions and answer for professional IT certifications. Take this examination content and scopes: \\nThe Terraform Associate 004 exam covers eight domains. First, Infrastructure as Code (IaC) with Terraform involves explaining what IaC is, describing the advantages of IaC patterns, and explaining how Terraform manages multi-cloud, hybrid cloud, and service-agnostic workflows. Second, Terraform fundamentals requires installing and versioning Terraform providers, describing how Terraform uses providers, writing configuration using multiple providers, and explaining how Terraform uses and manages state. Third, the Core Terraform workflow includes describing the workflow, initializing a working directory, validating configuration, generating and reviewing an execution plan, applying changes, destroying managed infrastructure, and applying formatting and style adjustments to a configuration. Fourth, Terraform configuration focuses on using and differentiating resource and data blocks, referring to resource attributes and creating cross-resource references, using variables and outputs, understanding and using complex types, writing dynamic configuration using expressions and functions, defining resource dependencies, validating configuration using custom conditions, and understanding best practices for managing sensitive data, including secrets management with Vault. Fifth, Terraform modules covers explaining how Terraform sources modules, describing variable scope within modules, using modules in configuration, and managing module versions. Sixth, Terraform state management addresses describing the local backend, describing state locking, configuring remote state using the backend block, and managing resource drift and Terraform state. Seventh, Maintain infrastructure with Terraform involves importing existing infrastructure, using the CLI to inspect state, and describing when and how to use verbose logging. Eighth, HCP Terraform covers using HCP Terraform to create infrastructure, describing its collaboration and governance features, describing how to organize and use workspaces and projects, and configuring and using HCP Terraform integration.Generate 16 high-quality and non-duplicated multiple-choice questions (there are 2 questions per each domain) with answers and explanations for the HashiCorp Certified: Terraform Associate 004 examination.Each question is more than 60 words in length. Some questions may relevant to real-world scenarios or definitions or coding skills. The difficult level should be evenly moderate or difficult. Please provide a response in a structured JSON format with the key name \"questions\", including all explanations for each answer as a valid JSON object. Each explanation has more than 50 words. The response questions and answers should not contain single or double quote characters so that it must avoid the error while parsing JSON format \"Error decoding JSON: Expecting property name enclosed in double quotes.\"Sample response structure should be like this: {\"type\": \"multiple-choice\", \"exported\":0, \"question\": \"a text\", \"options\": { \"A\": \"a text\", \"B\": \"a text\", \"C\": \"a text\", \"D\": \"a text\"}, \"answer\": B, \"explanation\": { \"A\": \"a text\", \"B\": \"a text\", \"C\": \"a text\", \"D\": \"a text\"}}.\n",
      "TERRA_A_004 ========== Finish generating set: 1\n"
     ]
    }
   ],
   "source": [
    "#run it: python generate_questions.py\n",
    "cert_symbol = 'TERRA_A_004' #predefined in db (create new folder in this project in advance)\n",
    "platform = ''\n",
    "begin_generate_questions(cert_symbol, platform, 1)    #ideally 6 full tests\n",
    "\n",
    "#generate CSV files\n",
    "# for i in range(1,7):  \n",
    "#     begin_export_csv(cert_symbol, str(i))    #Practice set index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
