{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pymongo\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from math import ceil\n",
    "import csv\n",
    "import importlib\n",
    "# Get the current working directory of the notebook\n",
    "notebook_dir = os.getcwd()\n",
    "# Get the path to the parent directory\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "\n",
    "# Add the parent directory to sys.path if it's not already there\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "import const\n",
    "# importlib.reload(const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True) \n",
    "GENERATIVE_URI = os.environ['GENERATIVE_URI']\n",
    "db_client = pymongo.MongoClient(os.environ['DB_URI'])\n",
    "db = db_client['db_certificates']   \n",
    "metadata_collection = db['tb_cert_metadata']    #meta data of certificates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(GENERATIVE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLE_PROMPT = os.environ['ROLE_PROMPT']\n",
    "COMMON_QUESTION_PROMPT = os.environ['COMMON_QUESTION_PROMPT']\n",
    "MULTI_CHOICE_PROMPT = COMMON_QUESTION_PROMPT + os.environ['MULTI_CHOICE_PROMPT']\n",
    "MULTI_SELECTION_PROMPT = COMMON_QUESTION_PROMPT + os.environ['MULTI_SELECTION_PROMPT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPEN_ROUTER_AI_KEY=os.environ['OPENROUTER_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_questions_2_db(platform, collection, raw_questions, question_type):\n",
    "    questions = const.extract_questions_from_candidates(platform, raw_questions)\n",
    "    if questions:\n",
    "        #parse questions and answers\n",
    "        question_num = 0\n",
    "        for q in questions:\n",
    "            if question_type == 'multiple-choice' or (len(q['answer']) > 1):\n",
    "                q['exported'] = 0\n",
    "                q['uuid'] = const.generate_random_uuid()\n",
    "                #print(q)\n",
    "                if 'explanation' in q and 'answer' in q and 'question' in q and 'options' in q:\n",
    "                    if 'A' in q['explanation'] and 'A' in q['options']:\n",
    "                        const.insert_questions(collection, q)\n",
    "                        question_num += 1\n",
    "        print('Stored ' + str(question_num) + ' questions to db successfully')\n",
    "    else:\n",
    "        print(\"Error: No questions found in the parsed content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(cert_metadata, platform):\n",
    "    if 'prompt_context' not in cert_metadata:\n",
    "        print('Missing prompt_context')\n",
    "        return\n",
    "    context = cert_metadata['prompt_context']\n",
    "    exam_name = cert_metadata['name']\n",
    "\n",
    "    question_collection = db[cert_metadata['collection_name']]\n",
    "    exceeded_quota = False\n",
    "    #multiple choice\n",
    "    if 'multi_choice_prompt_prefix' in cert_metadata:\n",
    "        text_prompt = cert_metadata['multi_choice_prompt_prefix'].replace('{exam_name}', exam_name) + MULTI_CHOICE_PROMPT\n",
    "        final_prompt = ROLE_PROMPT + context + text_prompt\n",
    "        no_of_loop = ceil(cert_metadata['multi_choice_questions'] / 10)\n",
    "        for i in range(no_of_loop):\n",
    "            if platform is None or platform == '':\n",
    "                #default is Gemini\n",
    "                raw_generated_text = const.post_request_generative_ai(GENERATIVE_URI, final_prompt)\n",
    "                if 'error' in raw_generated_text and 'message' in raw_generated_text['error']:\n",
    "                    if raw_generated_text['error']['message'].find('You exceeded your current quota') >= 0:\n",
    "                        print('You exceeded your current quota, pls try other key or wait until next day')\n",
    "                        exceeded_quota = True\n",
    "                        break\n",
    "            elif platform == 'OPENROUTER':\n",
    "                raw_generated_text = const.send_raw_request_2_openrouter(final_prompt, OPEN_ROUTER_AI_KEY)\n",
    "            \n",
    "            store_questions_2_db(platform, question_collection, raw_generated_text, 'multiple-choice')\n",
    "            time.sleep(5)   #delay 5 seconds\n",
    "    #multi selection, if any\n",
    "    if exceeded_quota == False and 'multi_selection_prompt_prefix' in cert_metadata:\n",
    "        text_prompt = cert_metadata['multi_selection_prompt_prefix'].replace('{exam_name}', exam_name) + MULTI_SELECTION_PROMPT\n",
    "        final_prompt = ROLE_PROMPT + context + text_prompt\n",
    "        no_of_loop = ceil(cert_metadata['multi_selection_questions'] / 10)\n",
    "        for i in range(no_of_loop):\n",
    "            raw_generated_text = const.post_request_generative_ai(GENERATIVE_URI, final_prompt)\n",
    "            if 'error' in raw_generated_text and 'message' in raw_generated_text['error']:\n",
    "                if raw_generated_text['error']['message'].find('You exceeded your current quota') >= 0:\n",
    "                    print('You exceeded your current quota, pls try other key or wait until next day')\n",
    "                    break\n",
    "            store_questions_2_db(question_collection, raw_generated_text, 'multiple-selection')\n",
    "            time.sleep(5)   #delay 5 seconds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def begin_generate_questions(cert_symbol, platform, no_of_tests):\n",
    "    if cert_symbol is None or cert_symbol == '':\n",
    "        return\n",
    "    #query metadata of this symbol\n",
    "    cert_metadata = metadata_collection.find_one({'symbol': cert_symbol})\n",
    "    if cert_metadata is None:\n",
    "        print('Certificate not found')\n",
    "        return\n",
    "    print('Begin generating questions for: ' + cert_metadata['name'])\n",
    "    #\n",
    "    for i in range(no_of_tests):\n",
    "        generate_questions(cert_metadata, platform)\n",
    "        print(cert_symbol + ' ========== Finish generating set: ' + str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_csv(cert_metadata, test_set_number):\n",
    "    question_collection = db[cert_metadata['collection_name']]\n",
    "    file_path = './'+cert_metadata['collection_name']+'/'\n",
    "    #get questions that not exported yet. Note that: each part must follow by domain percents\n",
    "    file_data = []\n",
    "    #append header line (both multi-choice and multi-selection)\n",
    "    file_data.append(['Question','Question Type','Answer Option 1','Explanation 1','Answer Option 2','Explanation 2','Answer Option 3','Explanation 3','Answer Option 4','Explanation 4','Answer Option 5','Explanation 5','Answer Option 6','Explanation 6','Correct Answers','Overall Explanation','Domain'])\n",
    "    exported_uuid = []\n",
    "    manual_uuid = []\n",
    "    #1. export multiple-choice first\n",
    "    pipeline = [\n",
    "                {\"$match\": {'exported': 0, 'type': 'multiple-choice'}},\n",
    "                {\"$sample\": {\"size\": cert_metadata['multi_choice_questions']}}\n",
    "            ]\n",
    "    random_documents = list(question_collection.aggregate(pipeline))\n",
    "    for doc in random_documents:\n",
    "        # print(doc)\n",
    "        if 'D' not in doc['options']:\n",
    "            print(doc['options'])\n",
    "        file_data.append([doc['question'].replace('  ', ' ').replace('\\n', ''), 'multiple-choice', \n",
    "                                  doc['options']['A'], doc['explanation']['A'].replace('  ', ' ').replace('\\n', ''),     #A\n",
    "                                  doc['options']['B'], doc['explanation']['B'].replace('  ', ' ').replace('\\n', ''),     #B\n",
    "                                  doc['options']['C'], doc['explanation']['C'].replace('  ', ' ').replace('\\n', ''),     #C\n",
    "                                  doc['options']['D'], doc['explanation']['D'].replace('  ', ' ').replace('\\n', ''),     #D\n",
    "                                  '', '',   #E\n",
    "                                  '', '',   #6\n",
    "                                  const.map_index(doc['answer']), #correct answer\n",
    "                                  '', #overall\n",
    "                                  '' #domain\n",
    "                                  ])\n",
    "        exported_uuid.append(doc['uuid'])\n",
    "    #2. multi selection\n",
    "    if 'multi_selection_questions' in cert_metadata and cert_metadata['multi_selection_questions'] > 0:\n",
    "        pipeline = [\n",
    "                    {\"$match\": {'exported': 0, 'type': 'multiple-selection'}},\n",
    "                    {\"$sample\": {\"size\": cert_metadata['multi_selection_questions']}}\n",
    "                ]\n",
    "        random_documents = list(question_collection.aggregate(pipeline))\n",
    "        for doc in random_documents:\n",
    "            exported_uuid.append(doc['uuid'])\n",
    "            manual_uuid.append(doc['uuid']) #they do not suppor bulk upload this type of question, we need to manually add them\n",
    "    #save all questions to csv\n",
    "    filename = cert_metadata['csv_filename_prefix']+test_set_number+'.csv'\n",
    "    try:\n",
    "        with open(file_path + filename, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerows(file_data)\n",
    "            print(f\"Data successfully saved to '{file_path}{filename}'\")\n",
    "            for _id in exported_uuid:\n",
    "                question_collection.update_one({'uuid': _id}, {'$set': {'exported': 1, 'filename': filename}})\n",
    "            print('\",\"'.join(manual_uuid))\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the array: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export 1 test at once\n",
    "def begin_export_csv(cert_symbol, test_set_number):\n",
    "    if cert_symbol is None or cert_symbol == '':\n",
    "        return\n",
    "    cert_metadata = metadata_collection.find_one({'symbol': cert_symbol})\n",
    "    if cert_metadata is None:\n",
    "        print('Certificate not found')\n",
    "        return\n",
    "    #\n",
    "    export_csv(cert_metadata, test_set_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run it: python generate_questions.py\n",
    "cert_symbol = 'DTB_GAEA' #predefined in db (create new folder in this project in advance)\n",
    "platform = 'OPENROUTER'\n",
    "# begin_generate_questions(cert_symbol, platform, 1)    #ideally 6 full tests\n",
    "\n",
    "#generate CSV files\n",
    "# for i in range(1,7):  \n",
    "#     begin_export_csv(cert_symbol, str(i))    #Practice set index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TERRA_A_004\n",
      "tb_terra_a_004\n",
      "========\n",
      "AZ_DP_300\n",
      "tb_az_dp_300\n",
      "========\n",
      "AZ_AZ_900\n",
      "tb_az_az_900\n",
      "========\n",
      "AZ_AZ_700\n",
      "tb_az_az_700\n",
      "========\n",
      "AZ_AZ_500\n",
      "tb_az_az_500\n",
      "========\n"
     ]
    }
   ],
   "source": [
    "#set flag to exams that have incorrect exported test\n",
    "def find_wrong_exams():\n",
    "    certs = metadata_collection.find({'udemy_link': {'$ne':None}})\n",
    "    for cert in certs:\n",
    "        question_collection = db[cert['collection_name']]\n",
    "        exported_questions = question_collection.find({'exported': 1})\n",
    "        if len(list(exported_questions)) < 240:\n",
    "            print(cert['symbol'])\n",
    "            print(cert['collection_name'])\n",
    "            print('========')\n",
    "#test\n",
    "find_wrong_exams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
