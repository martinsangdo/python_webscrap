{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pymongo\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from math import ceil\n",
    "import csv\n",
    "import importlib\n",
    "\n",
    "# Get the current working directory of the notebook\n",
    "notebook_dir = os.getcwd()\n",
    "# Get the path to the parent directory\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "\n",
    "# Add the parent directory to sys.path if it's not already there\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "import const\n",
    "#importlib.reload(const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True) \n",
    "GENERATIVE_URI = os.environ['GENERATIVE_URI']\n",
    "db_client = pymongo.MongoClient(os.environ['DB_URI'])\n",
    "db = db_client['db_certificates']   \n",
    "metadata_collection = db['tb_cert_metadata']    #meta data of certificates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(GENERATIVE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMON_QUESTION_PROMPT = os.environ['COMMON_QUESTION_PROMPT']\n",
    "MULTI_CHOICE_PROMPT = COMMON_QUESTION_PROMPT + os.environ['MULTI_CHOICE_PROMPT']\n",
    "MULTI_SELECTION_PROMPT = COMMON_QUESTION_PROMPT + os.environ['MULTI_SELECTION_PROMPT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_questions_2_db(collection, raw_questions, question_type):\n",
    "    questions = const.extract_questions_from_candidates(raw_questions)\n",
    "    if questions:\n",
    "        #parse questions and answers\n",
    "        for q in questions:\n",
    "            if question_type == 'multiple-choice' or (len(q['answer']) > 1):\n",
    "                q['exported'] = 0\n",
    "                q['uuid'] = const.generate_random_uuid()\n",
    "                #print(q)\n",
    "                const.insert_questions(collection, q)\n",
    "        print('Stored questions to db successfully')\n",
    "    else:\n",
    "        print(\"Error: No questions found in the parsed content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(cert_metadata):\n",
    "    if 'prompt_context' not in cert_metadata:\n",
    "        print('Missing prompt_context')\n",
    "        return\n",
    "    context = cert_metadata['prompt_context']\n",
    "    question_collection = db[cert_metadata['collection_name']]\n",
    "    exceeded_quota = False\n",
    "    #multiple choice\n",
    "    if 'multi_choice_prompt_prefix' in cert_metadata:\n",
    "        text_prompt = cert_metadata['multi_choice_prompt_prefix'] + MULTI_CHOICE_PROMPT\n",
    "        final_prompt = context + text_prompt\n",
    "        no_of_loop = ceil(cert_metadata['multi_choice_questions'] / 10)\n",
    "        for i in range(no_of_loop):\n",
    "            raw_generated_text = const.post_request_generative_ai(GENERATIVE_URI, final_prompt)\n",
    "            if 'error' in raw_generated_text and 'message' in raw_generated_text['error']:\n",
    "                if raw_generated_text['error']['message'].find('You exceeded your current quota') >= 0:\n",
    "                    print('You exceeded your current quota, pls try other key or wait until next day')\n",
    "                    exceeded_quota = True\n",
    "                    break\n",
    "            store_questions_2_db(question_collection, raw_generated_text, 'multiple-choice')\n",
    "            time.sleep(5)   #delay 5 seconds\n",
    "    #multi selection, if any\n",
    "    if exceeded_quota == False and 'multi_selection_prompt_prefix' in cert_metadata:\n",
    "        text_prompt = cert_metadata['multi_selection_prompt_prefix'] + MULTI_SELECTION_PROMPT\n",
    "        final_prompt = context + text_prompt\n",
    "        no_of_loop = ceil(cert_metadata['multi_selection_questions'] / 10)\n",
    "        for i in range(no_of_loop):\n",
    "            raw_generated_text = const.post_request_generative_ai(GENERATIVE_URI, final_prompt)\n",
    "            if 'error' in raw_generated_text and 'message' in raw_generated_text['error']:\n",
    "                if raw_generated_text['error']['message'].find('You exceeded your current quota') >= 0:\n",
    "                    print('You exceeded your current quota, pls try other key or wait until next day')\n",
    "                    break\n",
    "            store_questions_2_db(question_collection, raw_generated_text, 'multiple-selection')\n",
    "            time.sleep(5)   #delay 5 seconds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def begin_generate_questions(cert_symbol, no_of_tests):\n",
    "    #query metadata of this symbol\n",
    "    cert_metadata = metadata_collection.find_one({'symbol': cert_symbol})\n",
    "    if cert_metadata is None:\n",
    "        print('Certificate not found')\n",
    "        return\n",
    "    print('Begin generating questions for: ' + cert_metadata['name'])\n",
    "    #\n",
    "    for i in range(no_of_tests):\n",
    "        generate_questions(cert_metadata)\n",
    "        print(cert_symbol + ' ========== Finish generating set: ' + str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AWS_SAA, AWS_SAP, AWS_CLF_C02, AWS_DVA_C02, AWS_MLA, AWS_DOP, PMI-ACP\n",
    "cert_symbol = 'PMI-ACP' #predefined\n",
    "\n",
    "#begin_generate_questions(cert_symbol, 2)    #ideally 6 full tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_csv(cert_metadata, test_set_number):\n",
    "    question_collection = db[cert_metadata['collection_name']]\n",
    "    file_path = './'+cert_metadata['collection_name']+'/'\n",
    "    #get questions that not exported yet. Note that: each part must follow by domain percents\n",
    "    file_data = []\n",
    "    #append header line (both multi-choice and multi-selection)\n",
    "    file_data.append(['Question','Question Type','Answer Option 1','Explanation 1','Answer Option 2','Explanation 2','Answer Option 3','Explanation 3','Answer Option 4','Explanation 4','Answer Option 5','Explanation 5','Answer Option 6','Explanation 6','Correct Answers','Overall Explanation','Domain'])\n",
    "    exported_uuid = []\n",
    "    manual_uuid = []\n",
    "    #1. export multiple-choice first\n",
    "    pipeline = [\n",
    "                {\"$match\": {'exported': 0, 'type': 'multiple-choice'}},\n",
    "                {\"$sample\": {\"size\": cert_metadata['multi_choice_questions']}}\n",
    "            ]\n",
    "    random_documents = list(question_collection.aggregate(pipeline))\n",
    "    for doc in random_documents:\n",
    "        file_data.append([doc['question'].replace('  ', ' ').replace('\\n', ''), 'multiple-choice', \n",
    "                                  doc['options']['A'], doc['explanation']['A'].replace('  ', ' ').replace('\\n', ''),     #A\n",
    "                                  doc['options']['B'], doc['explanation']['B'].replace('  ', ' ').replace('\\n', ''),     #B\n",
    "                                  doc['options']['C'], doc['explanation']['C'].replace('  ', ' ').replace('\\n', ''),     #C\n",
    "                                  doc['options']['D'], doc['explanation']['D'].replace('  ', ' ').replace('\\n', ''),     #D\n",
    "                                  '', '',   #E\n",
    "                                  '', '',   #6\n",
    "                                  const.map_index(doc['answer']), #correct answer\n",
    "                                  '', #overall\n",
    "                                  '' #domain\n",
    "                                  ])\n",
    "        exported_uuid.append(doc['uuid'])\n",
    "    #2. multi selection\n",
    "    if 'multi_selection_questions' in cert_metadata:\n",
    "        pipeline = [\n",
    "                    {\"$match\": {'exported': 0, 'type': 'multiple-selection'}},\n",
    "                    {\"$sample\": {\"size\": cert_metadata['multi_selection_questions']}}\n",
    "                ]\n",
    "        random_documents = list(question_collection.aggregate(pipeline))\n",
    "        for doc in random_documents:\n",
    "            exported_uuid.append(doc['uuid'])\n",
    "            manual_uuid.append(doc['uuid']) #they do not suppor bulk upload this type of question, we need to manually add them\n",
    "    #save all questions to csv\n",
    "    filename = cert_metadata['csv_filename_prefix']+test_set_number+'.csv'\n",
    "    try:\n",
    "        with open(file_path + filename, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerows(file_data)\n",
    "            print(f\"Data successfully saved to '{file_path}/{filename}'\")\n",
    "            for _id in exported_uuid:\n",
    "                question_collection.update_one({'uuid': _id}, {'$set': {'exported': 1, 'filename': filename}})\n",
    "            print('\",\"'.join(manual_uuid))\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the array: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to './tb_pmi_acp//tb_pmi_acp_q_6.csv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#export 1 test at once\n",
    "def begin_export_csv(cert_symbol, test_set_number):\n",
    "    cert_metadata = metadata_collection.find_one({'symbol': cert_symbol})\n",
    "    if cert_metadata is None:\n",
    "        print('Certificate not found')\n",
    "        return\n",
    "    #\n",
    "    export_csv(cert_metadata, test_set_number)\n",
    "    \n",
    "#test\n",
    "#begin_export_csv(cert_symbol, '6')    #Practice set index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
