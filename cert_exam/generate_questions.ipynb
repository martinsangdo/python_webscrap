{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import pymongo\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from math import ceil\n",
    "\n",
    "# Get the current working directory of the notebook\n",
    "notebook_dir = os.getcwd()\n",
    "# Get the path to the parent directory\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "\n",
    "# Add the parent directory to sys.path if it's not already there\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "import const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() \n",
    "GENERATIVE_URI = os.environ['GENERATIVE_URI']\n",
    "db_client = pymongo.MongoClient(os.environ['DB_URI'])\n",
    "db = db_client['db_certificates']   \n",
    "metadata_collection = db['tb_cert_metadata']    #meta data of certificates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMON_QUESTION_PROMPT = 'Each question is more than 60 words in length and should focus more on complex real-world scenarios rather than definitions. Please provide a response in a structured JSON format with the key name \"questions\", including all explanations for each answer as a JSON object. Each explanation has more than 50 words. The response should avoid error while parsing JSON format \"Error decoding JSON: Expecting property name enclosed in double quotes.\"'\n",
    "MULTI_CHOICE_PROMPT = COMMON_QUESTION_PROMPT + 'Sample response structure should be like this: {\"question\": \"a text\", \"options\": { \"A\": \"a text\", \"B\": \"a text\", \"C\": \"a text\", \"D\": \"a text\"}, \"answer\": B, \"explanation\": { \"A\": \"a text\", \"B\": \"a text\", \"C\": \"a text\", \"D\": \"a text\"}, type: \"multiple-choice\", domain: x}.'\n",
    "MULTI_SELECTION_PROMPT = COMMON_QUESTION_PROMPT + 'Sample response structure should be like this: { \"question\" : \"a text\", \"options\" : { \"A\" : \"a text\", \"B\" : \"a text\", \"C\" : \"a text\", \"D\" : \"a text\", \"E\": \"a_text\"},\"answer\" : [],\"explanation\" : { \"A\" : \"a text\", \"B\" : \"a text\", \"C\" : \"a text\", \"D\" : \"a text\", \"E\" : \"a text\"},\"type\" : \"multiple-selection\",\"domain\":x}. Do not generate a question that has only 1 correct answer. The array \"answer\" should have 2 or 3 correct elements.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_questions_2_db(collection, raw_questions):\n",
    "    questions = const.extract_questions_from_candidates(raw_questions)\n",
    "    if questions:\n",
    "        #parse questions and answers\n",
    "        for q in questions:\n",
    "            q['exported'] = 0\n",
    "            q['uuid'] = const.generate_random_uuid()\n",
    "            #print(q)\n",
    "            const.insert_questions(collection, q)\n",
    "        print('Stored questions to db successfully')\n",
    "    else:\n",
    "        print(\"Error: No questions found in the parsed content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(cert_symbol, cert_metadata):\n",
    "    context = cert_metadata['prompt_context']\n",
    "    question_collection = db[cert_metadata['collection_name']]\n",
    "    #multiple choice\n",
    "    if 'multi_choice_prompt_prefix' in cert_metadata:\n",
    "        text_prompt = cert_metadata['multi_choice_prompt_prefix'] + MULTI_CHOICE_PROMPT\n",
    "        final_prompt = context + text_prompt\n",
    "        no_of_loop = ceil(cert_metadata['multi_choice_questions'] / 10)\n",
    "        for i in range(no_of_loop):\n",
    "            raw_generated_text = const.post_request_generative_ai(GENERATIVE_URI, final_prompt)\n",
    "            store_questions_2_db(question_collection, raw_generated_text)\n",
    "            time.sleep(5)   #delay 5 seconds\n",
    "    #multi selection, if any\n",
    "    if 'multi_selection_prompt_prefix' in cert_metadata:\n",
    "        text_prompt = cert_metadata['multi_selection_prompt_prefix'] + MULTI_SELECTION_PROMPT\n",
    "        final_prompt = context + text_prompt\n",
    "        no_of_loop = ceil(cert_metadata['multi_selection_questions'] / 10)\n",
    "        for i in range(no_of_loop):\n",
    "            raw_generated_text = const.post_request_generative_ai(GENERATIVE_URI, final_prompt)\n",
    "            store_questions_2_db(question_collection, raw_generated_text)\n",
    "            time.sleep(5)   #delay 5 seconds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def begin_generate_questions(cert_symbol, no_of_tests):\n",
    "    #query metadata of this symbol\n",
    "    cert_metadata = metadata_collection.find_one({'symbol': cert_symbol})\n",
    "    if cert_metadata is None:\n",
    "        print('Certificate not found')\n",
    "        return\n",
    "    print('Begin generating questions for: ' + cert_metadata['name'])\n",
    "    #\n",
    "    for i in range(no_of_tests):\n",
    "        generate_questions(cert_symbol, cert_metadata)\n",
    "        print(cert_symbol + ' ========== Finish generating set: ' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin generating questions of: AWS Certified Developer - Associate (DVA-C02)\n",
      "Stored questions to db successfully\n",
      "AWS_DVA_C02 ========== Finish generating set: 0\n"
     ]
    }
   ],
   "source": [
    "cert_symbol = 'AWS_DVA_C02' #predefined\n",
    "begin_generate_questions(cert_symbol, 1)    #ideally 6 full tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
