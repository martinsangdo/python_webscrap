{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import pymongo\n",
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "import csv\n",
    "\n",
    "# Get the current working directory of the notebook\n",
    "notebook_dir = os.getcwd()\n",
    "# Get the path to the parent directory\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "\n",
    "# Add the parent directory to sys.path if it's not already there\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "import const\n",
    "#importlib.reload(const)    #if we update the file const.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() \n",
    "GENERATIVE_URI = os.environ['GENERATIVE_URI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_client = pymongo.MongoClient(os.environ['DB_URI'])\n",
    "db = db_client['db_certificates']\n",
    "collection = db['tb_aws_cloud_practitioner']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exam has the following content domains and weightings:\n",
    "• Domain 1: Cloud Concepts (24% of scored content)\n",
    "• Domain 2: Security and Compliance (30% of scored content)\n",
    "• Domain 3: Cloud Technology and Services (34% of scored content)\n",
    "• Domain 4: Billing, Pricing, and Support (12% of scored content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Finish loop: 0\n",
      "===== Finish loop: 1\n",
      "===== Finish loop: 2\n",
      "===== Finish loop: 3\n",
      "===== Finish loop: 4\n",
      "===== Finish loop: 5\n",
      "===== Finish loop: 6\n",
      "===== Finish loop: 7\n",
      "===== Finish loop: 8\n",
      "===== Finish loop: 9\n",
      "===== Finish loop: 10\n",
      "===== Finish loop: 11\n",
      "===== Finish loop: 12\n",
      "===== Finish loop: 13\n",
      "===== Finish loop: 14\n",
      "===== Finish loop: 15\n",
      "===== Finish loop: 16\n",
      "===== Finish loop: 17\n",
      "===== Finish loop: 18\n",
      "===== Finish loop: 19\n",
      "===== Finish loop: 20\n",
      "===== Finish loop: 21\n",
      "===== Finish loop: 22\n",
      "===== Finish loop: 23\n",
      "===== Finish loop: 24\n",
      "===== Finish loop: 25\n",
      "===== Finish loop: 26\n",
      "===== Finish loop: 27\n",
      "===== Finish loop: 28\n",
      "===== Finish loop: 29\n",
      "===== Finish loop: 30\n",
      "===== Finish loop: 31\n",
      "Error decoding JSON: Expecting property name enclosed in double quotes: line 82 column 7 (char 6784)\n",
      "{'candidates': [{'content': {'parts': [{'text': '```json\\n{\\n  \"questions\": [\\n    {\\n      \"question\": \"Acme Corp., a mid-sized manufacturing company, is considering migrating its on-premises infrastructure to AWS.  Their current system includes a legacy CRM application running on aging hardware, a newly developed web application handling customer orders, and a large database of customer and product information.  Which migration strategy would be MOST suitable to minimize downtime and risk while allowing them to gradually transition their applications to the cloud?\",\\n      \"options\": {\\n        \"A\": \"Rehosting (Lift and Shift): Migrate all applications and data to AWS without significant changes.\",\\n        \"B\": \"Refactoring:  Completely rewrite all applications to leverage AWS services and optimize for cloud-native architecture.\",\\n        \"C\": \"Repurchasing: Replace existing applications with SaaS solutions offered by AWS or other providers.\",\\n        \"D\": \"Hybrid Approach: Migrate the web application and database to AWS first, while retaining the legacy CRM on-premises for a period of time.\"\\n      },\\n      \"answer\": \"D\",\\n      \"explanation\": {\\n        \"A\": \"While rehosting is simple, it doesn\\'t address the challenges of the aging hardware or the opportunity for optimization.  It may also lead to higher costs in the long run.\",\\n        \"B\": \"Refactoring is a significant undertaking and risks delaying functionality and creating considerable development time and expense.  It\\'s not suitable for a quick migration.\",\\n        \"C\": \"Repurchasing might be suitable for the web application but less so for the legacy CRM and database; it requires a complete functional evaluation and replacement, which is costly and time-consuming.\",\\n        \"D\": \"A hybrid approach allows Acme to prioritize the migration of the more crucial and adaptable web application and database to AWS while gradually decommissioning the legacy CRM system at a controlled pace, minimizing risk and ensuring business continuity.\"\\n      },\\n      \"type\": \"multiple-choice\",\\n      \"domain\": 1\\n    },\\n    {\\n      \"question\": \"Your company is experiencing unpredictable spikes in website traffic during promotional campaigns.  Which AWS cost optimization strategy would be MOST effective in managing these fluctuations while ensuring application performance during peak periods?\",\\n      \"options\": {\\n        \"A\": \"Always maintain maximum capacity to handle peak loads.\",\\n        \"B\": \"Utilize Reserved Instances (RIs) for all compute resources.\",\\n        \"C\": \"Leverage AWS Auto Scaling and Spot Instances.\",\\n        \"D\": \"Negotiate a fixed-price contract with AWS for all services.\"\\n      },\\n      \"answer\": \"C\",\\n      \"explanation\": {\\n        \"A\": \"Maintaining maximum capacity constantly is highly inefficient and expensive; resources remain underutilized most of the time.\",\\n        \"B\": \"RIs are cost-effective for consistently high usage, but they\\'re inflexible and unsuitable for unpredictable spikes in demand.\",\\n        \"C\": \"Auto Scaling automatically adjusts compute capacity based on real-time demand, ensuring application performance during peak periods.  Spot Instances offer significant cost savings by using spare compute capacity.\",\\n        \"D\": \"Fixed-price contracts lack the flexibility needed to address fluctuating demand, leading to potential overspending or underperformance.\"\\n      },\\n      \"type\": \"multiple-choice\",\\n      \"domain\": 1\\n    },\\n    {\\n      \"question\": \"A security audit reveals that several employees have access to sensitive customer data they don\\'t require for their roles.  Which AWS access management capability would be MOST effective in addressing this issue and implementing the principle of least privilege?\",\\n      \"options\": {\\n        \"A\": \"Using a single IAM user account for all employees.\",\\n        \"B\": \"Implementing Identity and Access Management (IAM) roles and policies to grant only necessary permissions.\",\\n        \"C\": \"Reliance on AWS\\'s default security settings without further configuration.\",\\n        \"D\": \"Sharing the root account credentials among all administrators.\"\\n      },\\n      \"answer\": \"B\",\\n      \"explanation\": {\\n        \"A\": \"A single IAM user account for all employees creates a massive security risk. If that account is compromised, the entire system is at risk.\",\\n        \"B\": \"IAM roles and policies allow granular control over access permissions, aligning with the principle of least privilege. Each employee only receives access to necessary resources.\",\\n        \"C\": \"Relying on default settings is insufficient. It is crucial to configure policies based on specific security needs for each user and role.\",\\n        \"D\": \"Sharing root account credentials is extremely insecure. The root account has complete access, and any compromise would give attackers complete control.\"\\n      },\\n      \"type\": \"multiple-choice\",\\n      \"domain\": 2\\n    },\\n    {\\n      \"question\": \"Your company needs to store and process large volumes of unstructured data like images and videos, requiring high scalability and durability. Which AWS service is BEST suited for this purpose, considering cost-effectiveness and ease of management?\",\\n      \"options\": {\\n        \"A\": \"Amazon S3 (Simple Storage Service)\",\\n        \"B\": \"Amazon RDS (Relational Database Service)\",\\n        \"C\": \"Amazon EC2 (Elastic Compute Cloud)\",\\n        \"D\": \"Amazon DynamoDB (NoSQL database)\"\\n      },\\n      \"answer\": \"A\",\\n      \"explanation\": {\\n        \"A\": \"Amazon S3 is designed for storing and retrieving large amounts of unstructured data. It offers high scalability, durability, and cost-effectiveness.  It\\'s built for managing objects like images and videos.\",\\n        \"B\": \"Amazon RDS is suitable for structured data within relational databases, not the unstructured data described.\",\\n        \"C\": \"Amazon EC2 is a compute service; storing large amounts of data directly on EC2 instances is inefficient and expensive.\",\\n        \"D\": \"Amazon DynamoDB is ideal for structured and semi-structured data, better suited for applications requiring key-value or document databases than storing large media files.\"\\n      },\\n      \"type\": \"multiple-choice\",\\n      \"domain\": 3\\n    },\\n    {\\n      \"question\": \"An e-commerce company utilizes Amazon S3 for storing product images.  They need to ensure that only authorized personnel can access these images. Which security mechanism should they primarily implement?\",\\n      \"options\": {\\n        \"A\": \"Disable all public access for the S3 bucket.\",\\n        \"B\": \"Rely on AWS\\'s default S3 bucket permissions.\",\\n        \"C\": \"Use AWS KMS (Key Management Service) to encrypt all data but leave the bucket publicly accessible.\",\\n        \"D\": \"Implement an access control list (ACL) for the S3 bucket, ensuring fine-grained control over who can access specific files and folders.\",\\n      },\\n      \"answer\": \"A\",\\n      \"explanation\": {\\n        \"A\": \"Disabling public access is the fundamental first step in securing an S3 bucket.  It prevents unauthorized access to objects stored within the bucket.  Further granular controls, such as IAM policies, can be implemented.\",\\n        \"B\": \"Relying solely on default permissions is highly insecure, leaving the data potentially vulnerable.\",\\n        \"C\": \"Encryption is crucial for protecting data at rest, but leaving the bucket publicly accessible negates its security benefits.\",\\n        \"D\": \"ACLs provide granular control but disabling public access is essential before implementing ACLs for further refinement.  ACLs are less preferred than IAM-based access control for modern S3 buckets.\"\\n      },\\n      \"type\": \"multiple-choice\",\\n      \"domain\": 2\\n    },\\n    {\\n      \"question\": \"A company is using several AWS services, including EC2 instances, S3 storage, and RDS databases. They need a centralized dashboard to monitor their costs across all these services. Which AWS service is BEST suited for this purpose?\",\\n      \"options\": {\\n        \"A\": \"AWS Trusted Advisor\",\\n        \"B\": \"AWS CloudTrail\",\\n        \"C\": \"AWS Cost Explorer\",\\n        \"D\": \"AWS CloudFormation\"\\n      },\\n      \"answer\": \"C\",\\n      \"explanation\": {\\n        \"A\": \"AWS Trusted Advisor provides recommendations for improving the performance, security, and cost-efficiency of your AWS environment, but it doesn\\'t directly provide a centralized cost dashboard.\",\\n        \"B\": \"AWS CloudTrail is a service that provides a log of AWS API calls.  It is helpful for auditing but does not directly provide cost visibility.\",\\n        \"C\": \"AWS Cost Explorer offers a comprehensive view of your AWS costs across different services, allowing you to analyze spending trends, identify cost drivers, and optimize your cloud expenditure.\",\\n        \"D\": \"AWS CloudFormation is for infrastructure as code, allowing you to automate the provisioning of AWS resources. It\\'s not relevant to cost monitoring.\"\\n      },\\n      \"type\": \"multiple-choice\",\\n      \"domain\": 3\\n    },\\n    {\\n      \"question\": \"A startup is launching a new application on AWS.  They expect high initial growth but aren\\'t sure about their long-term resource needs. What AWS pricing model would be MOST cost-effective for their unpredictable workload?\",\\n      \"options\": {\\n        \"A\": \"Reserved Instances (RIs)\",\\n        \"B\": \"Savings Plans\",\\n        \"C\": \"On-Demand Instances\",\\n        \"D\": \"Spot Instances\"\\n      },\\n      \"answer\": \"C\",\\n      \"explanation\": {\\n        \"A\": \"RIs require a long-term commitment, which is risky for a startup with uncertain future needs.\",\\n        \"B\": \"Savings Plans provide discounts on compute usage, but they also involve commitments, making them potentially less flexible for startups.\",\\n        \"C\": \"On-demand instances offer the highest flexibility, paying only for what you use, which suits a startup\\'s uncertain future resource demands.\",\\n        \"D\": \"Spot instances are the most cost-effective option, but they also carry the risk of interruption if the underlying EC2 instances are reclaimed by AWS.\"\\n      },\\n      \"type\": \"multiple-choice\",\\n      \"domain\": 4\\n    },\\n    {\\n      \"question\": \"Your company is experiencing unexpected cost increases in its AWS environment. Which AWS service provides tools to set budgets, track spending, and receive alerts when cost thresholds are exceeded?\",\\n      \"options\": {\\n        \"A\": \"AWS Trusted Advisor\",\\n        \"B\": \"AWS Cost Explorer\",\\n        \"C\": \"AWS Budgets\",\\n        \"D\": \"AWS CloudWatch\"\\n      },\\n      \"answer\": \"C\",\\n      \"explanation\": {\\n        \"A\": \"AWS Trusted Advisor helps with cost optimization recommendations but doesn\\'t directly manage budgets or alerts.\",\\n        \"B\": \"AWS Cost Explorer shows your cost trends and usage patterns, but it doesn\\'t allow you to directly set budget limits or receive alerts.\",\\n        \"C\": \"AWS Budgets allows you to set custom budgets, track spending against those budgets, and get alerts when your costs exceed the defined thresholds.\",\\n        \"D\": \"AWS CloudWatch monitors metrics and logs, but it\\'s not specifically designed for budget management or cost alerts.\"\\n      },\\n      \"type\": \"multiple-choice\",\\n      \"domain\": 4\\n    },\\n    {\\n      \"question\": \"A company\\'s development team needs a managed database service that offers high availability and scalability for their web application. They need to choose between Amazon RDS for MySQL and Amazon Aurora.  Considering factors like cost, performance, and ease of management, which option is generally preferred for a new application requiring high availability and scalability?\",\\n      \"options\": {\\n        \"A\": \"Amazon RDS for MySQL, as it is a well-established, widely used database.\",\\n        \"B\": \"Amazon Aurora, as it is a MySQL-compatible service offering better performance and scalability compared to RDS for MySQL.\",\\n        \"C\": \"Both are equally suitable, the choice depends on other factors unrelated to scalability or performance.\",\\n        \"D\": \"Neither is a good choice; it\\'s better to build a custom database solution.\"\\n      },\\n      \"answer\": \"B\",\\n      \"explanation\": {\\n        \"A\": \"While RDS for MySQL is reliable, Aurora generally offers superior performance and scalability, especially for applications requiring high availability.  Aurora\\'s architecture is designed for better performance and redundancy.\",\\n        \"B\": \"Amazon Aurora is designed for high availability and scalability, often outperforming traditional MySQL instances.  It also offers features that simplify management and improve overall efficiency.\",\\n        \"C\": \"While both are managed services, Aurora is generally preferred for new applications needing high scalability and availability due to its enhanced architecture and features.\",\\n        \"D\": \"Building a custom database solution requires extensive development effort, significant expertise, and carries increased operational overhead compared to leveraging a managed service like Aurora.\"\\n      },\\n      \"type\": \"multiple-choice\",\\n      \"domain\": 3\\n    },\\n    {\\n      \"question\": \"A financial institution is migrating its critical applications to AWS.  They have stringent regulatory compliance requirements, including data residency restrictions.  Which AWS concept helps them meet these regulations by defining where their data is stored and processed?\",\\n      \"options\": {\\n        \"A\": \"AWS Free Tier\",\\n        \"B\": \"AWS Shared Responsibility Model\",\\n        \"C\": \"AWS Regions and Availability Zones\",\\n        \"D\": \"AWS CloudFormation\"\\n      },\\n      \"answer\": \"C\",\\n      \"explanation\": {\\n        \"A\": \"The AWS Free Tier is a promotional offer for new users, not related to data residency or compliance.\",\\n        \"B\": \"The Shared Responsibility Model divides security responsibilities between AWS and the customer, but it doesn\\'t address data location directly.\",\\n        \"C\": \"AWS Regions and Availability Zones are geographic locations where AWS data centers are situated. Choosing a specific region allows the institution to comply with data residency requirements.\",\\n        \"D\": \"AWS CloudFormation is a tool for infrastructure provisioning; it is not directly related to data residency regulations.\"\\n      },\\n      \"type\": \"multiple-choice\",\\n      \"domain\": 2\\n    }\\n  ]\\n}\\n```\\n'}], 'role': 'model'}, 'finishReason': 'STOP', 'avgLogprobs': -0.4013505268790276}], 'usageMetadata': {'promptTokenCount': 460, 'candidatesTokenCount': 3026, 'totalTokenCount': 3486, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 460}], 'candidatesTokensDetails': [{'modality': 'TEXT', 'tokenCount': 3026}]}, 'modelVersion': 'gemini-1.5-flash'}\n",
      "No questions found in the parsed content\n",
      "===== Finish loop: 32\n",
      "===== Finish loop: 33\n",
      "===== Finish loop: 34\n"
     ]
    }
   ],
   "source": [
    "#60 questions (60 multi choice, 5 multi select)\n",
    "def generate_questions():\n",
    "    context = 'The AWS Cloud Practitioner Exam has the following content domains: Domain 1 is Cloud Concepts, that includes: Define the benefits of the AWS Cloud; Identify design principles of the AWS Cloud; Understand the benefits of and strategies for migration to the AWS Cloud; Understand concepts of cloud economics. Domain 2 is Security and Compliance, that includes: Understand the AWS shared responsibility model; Understand AWS Cloud security, governance, and compliance concepts; Identify AWS access management capabilities; Identify components and resources for security. Domain 3 is Cloud Technology and Services, that includes: Define methods of deploying and operating in the AWS Cloud; Define the AWS global infrastructure; Identify AWS compute services; Identify AWS database services; Identify AWS network services; Identify AWS storage services; Identify AWS artificial intelligence and machine learning (AI/ML) services and analytics services; Identify services from other in-scope AWS service categories. Domain 4 is Billing, Pricing, and Support, that includes: Compare AWS pricing models; Understand resources for billing, budget, and cost management; Identify AWS technical resources and AWS Support options.'\n",
    "    #multiple choice\n",
    "    text_prompt = 'Generate 10 high-quality multiple-choice questions (2 questions in domain 1, 3 questions in domain 2, 3 questions in domain 3, 2 questions in domain 4) with answers and explanations for the AWS Cloud Practitioner examination. Each question has more than 60 words in length and should focus more on complex real-world scenarios rather than definitions. Please provide a response in a structured JSON format with the key name \"questions\", including all explanations for each answer as a JSON object. Each explanation has more than 50 words. Sample response structure should like this: { \"question\" : \"xxx\", \"options\" : { \"A\" : \"a text\", \"B\" : \"a text\", \"C\" : \"a text\", \"D\" : \"a text\"},\"answer\" : \"B\",\"explanation\" : { \"A\" : \"a text\", \"B\" : \"a text\", \"C\" : \"a text\", \"D\" : \"a text\"},\"type\" : \"multiple-choice\",\"domain\":x}. Response should avoid error while parsing JSON format \"Error decoding JSON: Expecting property name enclosed in double quotes\".'\n",
    "    #multi selection\n",
    "    # text_prompt = 'Generate 10 high-quality multiple-selection questions (2 questions in domain 1, 3 questions in domain 2, 3 questions in domain 3, 2 questions in domain 4) with answers and explanations for the AWS Cloud Practitioner examination examination. Each question has more than 60 words in length and should focus more on complex real-world scenarios rather than definitions. Please provide a response in a structured JSON format with the key name \"questions\", including all explanations for each answer as a JSON object. Each explanation has more than 50 words. Sample response structure should like this: { \"question\" : \"xxx\", \"options\" : { \"A\" : \"a text\", \"B\" : \"a text\", \"C\" : \"a text\", \"D\" : \"a text\", \"E\": \"a_text\"},\"answer\" : [],\"explanation\" : { \"A\" : \"a text\", \"B\" : \"a text\", \"C\" : \"a text\", \"D\" : \"a text\", \"E\" : \"a text\"},\"type\" : \"multiple-selection\",\"domain\":x}. \"answer\" should have 2 to 4 correct elements. Response should avoid error while parsing JSON format \"Error decoding JSON: Expecting property name enclosed in double quotes\".'\n",
    "    #\n",
    "    raw_generated_text = const.post_request_generative_ai(GENERATIVE_URI, context + text_prompt)\n",
    "\n",
    "    #print(raw_generated_text)\n",
    "    questions = const.extract_questions_from_candidates(raw_generated_text)\n",
    "    if questions:\n",
    "        #parse questions and answers\n",
    "        for q in questions:\n",
    "            q['exported'] = 0\n",
    "            q['uuid'] = const.generate_random_uuid()\n",
    "            #print(q)\n",
    "            const.insert_questions(collection, q)\n",
    "    else:\n",
    "        print(raw_generated_text)\n",
    "        print(\"No questions found in the parsed content\")\n",
    "#test\n",
    "for i in range(5):\n",
    "    generate_questions() #10 sentences 30 secs\n",
    "    print('===== Finish loop: ' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35d47290-1765-486e-9668-8eb8a8c5e6e2\",\"88dcddbf-340c-416c-9ae6-e80244119dba\",\"c2279d5f-0aa4-4491-945f-d8941b4f2011\",\"2ef8b57a-9ce6-472c-97cc-ce03e1bcea99\",\"09470d10-1bd5-4f84-8597-d3f274078e37\",\"200119d9-2875-4645-993c-a37f43623618\n",
      "Data successfully saved to 'gcp_data_eng_test_6_20250507.csv'\n"
     ]
    }
   ],
   "source": [
    "#Question,Question Type,Answer Option 1,Explanation 1,Answer Option 2,Explanation 2,Answer Option 3,Explanation 3,Answer Option 4,Explanation 4,Answer Option 5,Explanation 5,Answer Option 6,Explanation 6,Correct Answers,Overall Explanation,Domain\n",
    "def export_csv(path, filename):\n",
    "    #get questions that not exported yet. Note that: each part must follow by domain percents\n",
    "    file_data = []\n",
    "    #append header line (both multi-choice and multi-selection)\n",
    "    file_data.append(['Question','Question Type','Answer Option 1','Explanation 1','Answer Option 2','Explanation 2','Answer Option 3','Explanation 3','Answer Option 4','Explanation 4','Answer Option 5','Explanation 5','Answer Option 6','Explanation 6','Correct Answers','Overall Explanation','Domain'])\n",
    "    exported_uuid = []\n",
    "    manual_uuid = []\n",
    "    #1. export multiple-choice first\n",
    "    pipeline = [\n",
    "                {\"$match\": {'exported': 0, 'type': 'multiple-choice'}},\n",
    "                {\"$sample\": {\"size\": 54}}\n",
    "            ]\n",
    "    random_documents = list(collection.aggregate(pipeline))\n",
    "    for doc in random_documents:\n",
    "        file_data.append([doc['question'].replace('  ', ' ').replace('\\n', ''), 'multiple-choice', \n",
    "                                  doc['options']['A'], doc['explanation']['A'].replace('  ', ' ').replace('\\n', ''),     #A\n",
    "                                  doc['options']['B'], doc['explanation']['B'].replace('  ', ' ').replace('\\n', ''),     #B\n",
    "                                  doc['options']['C'], doc['explanation']['C'].replace('  ', ' ').replace('\\n', ''),     #C\n",
    "                                  doc['options']['D'], doc['explanation']['D'].replace('  ', ' ').replace('\\n', ''),     #D\n",
    "                                  '', '',   #E\n",
    "                                  '', '',   #6\n",
    "                                  const.map_index(doc['answer']), #correct answer\n",
    "                                  '', #overall\n",
    "                                  '' #domain\n",
    "                                  ])\n",
    "        exported_uuid.append(doc['uuid'])\n",
    "    #2. multi selection\n",
    "    pipeline = [\n",
    "                {\"$match\": {'exported': 0, 'type': 'multiple-selection'}},\n",
    "                {\"$sample\": {\"size\": 6}}\n",
    "            ]\n",
    "    random_documents = list(collection.aggregate(pipeline))\n",
    "    for doc in random_documents:\n",
    "        exported_uuid.append(doc['uuid'])\n",
    "        manual_uuid.append(doc['uuid']) #they do not suppor bulk upload this type of question, we need to manually add them\n",
    "    #\n",
    "    print('\",\"'.join(manual_uuid))\n",
    "    #save all questions to csv\n",
    "    try:\n",
    "        with open(path + filename, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerows(file_data)\n",
    "            print(f\"Data successfully saved to '{filename}'\")\n",
    "            for _id in exported_uuid:\n",
    "                collection.update_one({'uuid': _id}, {'$set': {'exported': 1, 'filename': filename}})\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the array: {e}\")\n",
    "    \n",
    "#test|\n",
    "export_csv('./gcp_data_engineer_data/', 'gcp_data_eng_test_6_20250507.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
