{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'const' from '/Users/sang/Documents/Source/Python/python_webscrap/const.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import pymongo\n",
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "import csv\n",
    "\n",
    "# Get the current working directory of the notebook\n",
    "notebook_dir = os.getcwd()\n",
    "# Get the path to the parent directory\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "\n",
    "# Add the parent directory to sys.path if it's not already there\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "import const\n",
    "#importlib.reload(const)    #if we update the file const.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() \n",
    "GENERATIVE_URI = os.environ['GENERATIVE_URI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_client = pymongo.MongoClient(os.environ['DB_URI'])\n",
    "db = db_client['db_certificates']\n",
    "collection = db['tb_gcp_data_enigneer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Section 1: Designing data processing systems (~22% of the exam)\n",
    "- Section 2: Ingesting and processing the data (~25% of the exam)\n",
    "- Section 3: Storing the data (~20% of the exam)\n",
    "- Section 4: Preparing and using data for analysis (~15% of the exam)\n",
    "- Section 5: Maintaining and automating data workloads (~18% of the exam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Finish loop: 0\n",
      "===== Finish loop: 1\n",
      "===== Finish loop: 2\n",
      "===== Finish loop: 3\n",
      "===== Finish loop: 4\n"
     ]
    }
   ],
   "source": [
    "#GCP I: 60 questions (54 multi choice, 6 multi select)\n",
    "def generate_questions():\n",
    "    context = 'GCP Google Cloud Professional Data Engineer certificate includes questions from the following sections: \"Section 1: Designing data processing systems (~22% of the exam):1.1 Designing for security and compliance. Considerations include:Identity and Access Management (e.g., Cloud IAM and organization policies).Data security (encryption and key management).Privacy (e.g., personally identifiable information, and Cloud Data Loss Prevention API).Regional considerations (data sovereignty) for data access and storage.Legal and regulatory compliance.1.2 Designing for reliability and fidelity. Considerations include:Preparing and cleaning data (e.g., Dataprep, Dataflow, and Cloud Data Fusion).Monitoring and orchestration of data pipelines.Disaster recovery and fault tolerance.Making decisions related to ACID (atomicity, consistency, isolation, and durability)compliance and availability.Data validation.1.3 Designing for flexibility and portability. Considerations include:Mapping current and future business requirements to the architecture.Designing for data and application portability (e.g., multi-cloud and data residencyrequirements).Data staging, cataloging, and discovery (data governance).1.4 Designing data migrations. Considerations include:Analyzing current stakeholder needs, users, processes, and technologies and creatinga plan to get to desired state.Planning migration to Google Cloud (e.g., BigQuery Data Transfer Service, Database Migration Service, Transfer Appliance, Google Cloud networking, Datastream).Designing the migration validation strategy.Designing the project, dataset, and table architecture to ensure proper datagovernance.Section 2: Ingesting and processing the data (~25% of the exam):2.1 Planning the data pipelines. Considerations include:Defining data sources and sinks.Defining data transformation logic.Networking fundamentals.Data encryption.2.2 Building the pipelines. Considerations include:Data cleansing.Identifying the services (e.g., Dataflow, Apache Beam, Dataproc, Cloud Data Fusion,BigQuery, Pub/Sub, Apache Spark, Hadoop ecosystem, and Apache Kafka).Transformations.Batch.Streaming (e.g., windowing, late arriving data).Language.Ad hoc data ingestion (one-time or automated pipeline).Data acquisition and import.Integrating with new data sources.2.3 Deploying and operationalizing the pipelines. Considerations include:Job automation and orchestration (e.g., Cloud Composer and Workflows).CI/CD (Continuous Integration and Continuous Deployment).Section 3: Storing the data (~20% of the exam):3.1 Selecting storage systems. Considerations include:Analyzing data access patterns.Choosing managed services (e.g., Bigtable, Spanner, Cloud SQL, Cloud Storage,Firestore, Memorystore).Planning for storage costs and performance.Lifecycle management of data.3.2 Planning for using a data warehouse. Considerations include:Designing the data model.Deciding the degree of data normalization.Mapping business requirements.Defining architecture to support data access patterns.3.3 Using a data lake. Considerations include:Managing the lake (configuring data discovery, access, and cost controls).Processing data.Monitoring the data lake.3.4 Designing for a data mesh. Considerations include:Building a data mesh based on requirements by using Google Cloud tools (e.g.,Dataplex, Data Catalog, BigQuery, Cloud Storage).Segmenting data for distributed team usage.Building a federated governance model for distributed data systems.Section 4: Preparing and using data for analysis (~15% of the exam):4.1 Preparing data for visualization. Considerations include:Connecting to tools.Precalculating fields.BigQuery materialized views (view logic).Determining granularity of time data.Troubleshooting poor performing queries.Identity and Access Management (IAM) and Cloud Data Loss Prevention (Cloud DLP).4.2 Sharing data. Considerations include:Defining rules to share data.Publishing datasets.Publishing reports and visualizations.Analytics Hub.4.3 Exploring and analyzing data. Considerations include:Preparing data for feature engineering (training and serving machine learning models).Conducting data discovery.Section 5: Maintaining and automating data workloads (~18% of the exam):5.1 Optimizing resources. Considerations include:Minimizing costs per required business need for data.Ensuring that enough resources are available for business-critical data processes.Deciding between persistent or job-based data clusters (e.g., Dataproc).5.2 Designing automation and repeatability. Considerations include:Creating directed acyclic graphs (DAGs) for Cloud Composer.Scheduling jobs in a repeatable way.5.3 Organizing workloads based on business requirements. Considerations include:Flex, on-demand, and flat rate slot pricing (index on flexibility or fixed capacity).Interactive or batch query jobs.5.4 Monitoring and troubleshooting processes. Considerations include:Observability of data processes (e.g., Cloud Monitoring, Cloud Logging, BigQueryadmin panel).Monitoring planned usage.Troubleshooting error messages, billing issues, and quotas.Manage workloads, such as jobs, queries, and compute capacity (reservations).5.5 Maintaining awareness of failures and mitigating impact. Considerations include:Designing system for fault tolerance and managing restarts.Running jobs in multiple regions or zones.Preparing for data corruption and missing data.Data replication and failover (e.g., Cloud SQL, Redis clusters).\"'\n",
    "    #multiple choice\n",
    "    #text_prompt = 'Generate 10 high-quality multiple-choice questions with answers and explanations for the Google Cloud Professional Data Engineer examination. Each question has more than 60 words in length and should focus more on complex real-world scenarios rather than definitions. Please provide a response in a structured JSON format with the key name \"questions\", including all explanations for each answer as a JSON object. Each explanation has more than 50 words. Sample response structure should like this: { \"question\" : \"xxx\", \"options\" : { \"A\" : \"a text\", \"B\" : \"a text\", \"C\" : \"a text\", \"D\" : \"a text\"},\"answer\" : \"B\",\"explanation\" : { \"A\" : \"a text\", \"B\" : \"a text\", \"C\" : \"a text\", \"D\" : \"a text\"},\"type\" : \"multiple-choice\"}. Response should avoid error while parsing JSON format \"Error decoding JSON: Expecting property name enclosed in double quotes\".'\n",
    "    #multi selection\n",
    "    text_prompt = 'Generate 10 high-quality multiple-selection questions with answers and explanations for the Google Cloud Professional Data Engineer examination. Each question has more than 60 words in length and should focus more on complex real-world scenarios rather than definitions. Please provide a response in a structured JSON format with the key name \"questions\", including all explanations for each answer as a JSON object. Each explanation has more than 50 words. Sample response structure should like this: { \"question\" : \"xxx\", \"options\" : { \"A\" : \"a text\", \"B\" : \"a text\", \"C\" : \"a text\", \"D\" : \"a text\", \"E\": \"a_text\"},\"answer\" : [],\"explanation\" : { \"A\" : \"a text\", \"B\" : \"a text\", \"C\" : \"a text\", \"D\" : \"a text\", \"E\" : \"a text\"},\"type\" : \"multiple-selection\"}. \"answer\" should have 2 to 4 correct elements. Response should avoid error while parsing JSON format \"Error decoding JSON: Expecting property name enclosed in double quotes\".'\n",
    "    #\n",
    "    raw_generated_text = const.post_request_generative_ai(GENERATIVE_URI, context + text_prompt)\n",
    "\n",
    "    #print(raw_generated_text)\n",
    "    questions = const.extract_questions_from_candidates(raw_generated_text)\n",
    "    if questions:\n",
    "        #parse questions and answers\n",
    "        for q in questions:\n",
    "            q['exported'] = 0\n",
    "            q['uuid'] = const.generate_random_uuid()\n",
    "            #print(q)\n",
    "            const.insert_questions(collection, q)\n",
    "    else:\n",
    "        print(raw_generated_text)\n",
    "        print(\"No questions found in the parsed content\")\n",
    "#test\n",
    "for i in range(5):\n",
    "    generate_questions() #10 sentences 30 secs\n",
    "    print('===== Finish loop: ' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35d47290-1765-486e-9668-8eb8a8c5e6e2\",\"88dcddbf-340c-416c-9ae6-e80244119dba\",\"c2279d5f-0aa4-4491-945f-d8941b4f2011\",\"2ef8b57a-9ce6-472c-97cc-ce03e1bcea99\",\"09470d10-1bd5-4f84-8597-d3f274078e37\",\"200119d9-2875-4645-993c-a37f43623618\n",
      "Data successfully saved to 'gcp_data_eng_test_6_20250507.csv'\n"
     ]
    }
   ],
   "source": [
    "#Question,Question Type,Answer Option 1,Explanation 1,Answer Option 2,Explanation 2,Answer Option 3,Explanation 3,Answer Option 4,Explanation 4,Answer Option 5,Explanation 5,Answer Option 6,Explanation 6,Correct Answers,Overall Explanation,Domain\n",
    "def export_csv(path, filename):\n",
    "    #get questions that not exported yet. Note that: each part must follow by domain percents\n",
    "    file_data = []\n",
    "    #append header line (both multi-choice and multi-selection)\n",
    "    file_data.append(['Question','Question Type','Answer Option 1','Explanation 1','Answer Option 2','Explanation 2','Answer Option 3','Explanation 3','Answer Option 4','Explanation 4','Answer Option 5','Explanation 5','Answer Option 6','Explanation 6','Correct Answers','Overall Explanation','Domain'])\n",
    "    exported_uuid = []\n",
    "    manual_uuid = []\n",
    "    #1. export multiple-choice first\n",
    "    pipeline = [\n",
    "                {\"$match\": {'exported': 0, 'type': 'multiple-choice'}},\n",
    "                {\"$sample\": {\"size\": 54}}\n",
    "            ]\n",
    "    random_documents = list(collection.aggregate(pipeline))\n",
    "    for doc in random_documents:\n",
    "        file_data.append([doc['question'].replace('  ', ' ').replace('\\n', ''), 'multiple-choice', \n",
    "                                  doc['options']['A'], doc['explanation']['A'].replace('  ', ' ').replace('\\n', ''),     #A\n",
    "                                  doc['options']['B'], doc['explanation']['B'].replace('  ', ' ').replace('\\n', ''),     #B\n",
    "                                  doc['options']['C'], doc['explanation']['C'].replace('  ', ' ').replace('\\n', ''),     #C\n",
    "                                  doc['options']['D'], doc['explanation']['D'].replace('  ', ' ').replace('\\n', ''),     #D\n",
    "                                  '', '',   #E\n",
    "                                  '', '',   #6\n",
    "                                  const.map_index(doc['answer']), #correct answer\n",
    "                                  '', #overall\n",
    "                                  '' #domain\n",
    "                                  ])\n",
    "        exported_uuid.append(doc['uuid'])\n",
    "    #2. multi selection\n",
    "    pipeline = [\n",
    "                {\"$match\": {'exported': 0, 'type': 'multiple-selection'}},\n",
    "                {\"$sample\": {\"size\": 6}}\n",
    "            ]\n",
    "    random_documents = list(collection.aggregate(pipeline))\n",
    "    for doc in random_documents:\n",
    "        exported_uuid.append(doc['uuid'])\n",
    "        manual_uuid.append(doc['uuid']) #they do not suppor bulk upload this type of question, we need to manually add them\n",
    "    #\n",
    "    print('\",\"'.join(manual_uuid))\n",
    "    #save all questions to csv\n",
    "    try:\n",
    "        with open(path + filename, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerows(file_data)\n",
    "            print(f\"Data successfully saved to '{filename}'\")\n",
    "            for _id in exported_uuid:\n",
    "                collection.update_one({'uuid': _id}, {'$set': {'exported': 1, 'filename': filename}})\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the array: {e}\")\n",
    "    \n",
    "#test|\n",
    "export_csv('./gcp_data_engineer_data/', 'gcp_data_eng_test_6_20250507.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
